Ссылка на Colab:

https://colab.research.google.com/drive/1m8eZHSR-44Uqlu5BEeCbQL4MqLEJAMp_?usp=sharing

# Лабораторная работа №3
Выполнила: Большова Елизавета Александровна

### Итоговые выводы

В ходе лабораторной работы было проведено два запуска по обучению сверточной нейронной сети на небольшом наборе данных "Dogs-Cats" 

![Набор данных](/NN_3_lab/image/example.png)

с целью изучения влияния аугментации данных на переобучение и обобщающую способность модели.

**Обучение без аугментации**

При обучении модели на исходных данных наблюдался классический эффект **сильного переобучения**. Уже после 5-10 эпох возник значительный разрыв между метриками на обучающей и валидационной выборках:
*   Точность на обучении достигла практически **100%**, в то время как точность на валидации остановилась на уровне **~75%**.
*   Потери на обучении стремились к нулю, а потери на валидации, снизившись вначале, затем начали расти.
Это свидетельствует о том, что модель не научилась обобщать признаки, а просто "запомнила" обучающие изображения.

![Пример без аугментации](/NN_3_lab/image/without_aug.jpg)

**Обучение с аугментацией и Dropout**

Во втором запуске для борьбы с переобучением были применены два метода:
1.  **Аугментация данных:** к обучающим изображениям применялись случайные преобразования (повороты, отражения, изменения цвета).

![Пример cats](/NN_3_lab/image/cats.jpg)

2.  **Регуляризация Dropout:** в архитектуру модели был добавлен слой Dropout с вероятностью 0.5.

Результаты показали **значительное улучшение**:
*   **Эффект переобучения был успешно снижен.** Разрыв между кривыми обучения и валидации стал минимальным, что видно на графиках.
*   Лучшая точность на валидационной выборке составила **82.75%** (на 60-й эпохе), что является существенным улучшением.
*   Кривые потерь на обучающей и валидационной выборках вели себя схожим образом, оставаясь близко друг к другу на протяжении всего процесса обучения. Это указывает на хорошую обобщающую способность модели.

![Пример c аугментацией](/NN_3_lab/image/with_aug.jpg)

### Заключение

Данная лабораторная работа наглядно демонстрирует, что аугментация данных является критически важным и эффективным инструментом при обучении нейронных сетей на ограниченных наборах данных. Она позволяет искусственно расширить обучающую выборку, заставляя модель изучать более робастные и общие признаки, что напрямую ведет к снижению переобучения и улучшению качества модели на новых, невиданных ранее данных.
