Ссылка на Colab:

https://colab.research.google.com/drive/1OFK4TG_vuiO9mRpWY-rNKcN4e3XJfVgL?usp=sharing

# Лабораторная работа №4
Выполнила: Большова Елизавета Александровна

### Выводы по результатам экспериментов

В ходе лабораторной работы было проведено сравнение двух подходов к решению задачи классификации изображений листьев на небольшом датасете: обучение собственной архитектуры ResNet с нуля и использование техники Transfer Learning на основе предобученной модели ResNet-18.

**Обучение SimpleResNet с нуля**

Обучение собственной реализации ResNet на ограниченном наборе данных показало крайне нестабильные результаты и сильное переобучение.
*   Точность на обучающей выборке быстро достигла >95%, в то время как точность на валидационной выборке сильно колебалась и не показывала стабильного роста.
*   График потерь на валидации демонстрировал хаотичные скачки, что свидетельствует о неспособности модели построить обобщающее представление о данных.
*   Этот эксперимент наглядно демонстрирует, что обучение глубоких архитектур с нуля требует очень больших объемов данных.

![Пример SimpleResNet](/NN_4_lab/image/result_SimpleResNet.jpg)

**Эксперимент 2: Transfer Learning с предобученной ResNet-18**

Использование предобученной на ImageNet модели с "заморозкой" всех слоев, кроме последнего, показало значительно лучшие результаты:
*   **Высокая начальная точность:** после первой эпохи точность на валидации составила **~94.5%**, что говорит об огромной пользе уже существующих в модели знаний.
*   **Быстрое и стабильное обучение:** достигла пиковой точности на валидации **~98%** всего за 5 эпох. Процесс обучения был стабильным, без резких колебаний.
*   **Отсутствие переобучения:** кривые точности и потерь для обучающей и валидационной выборок шли очень близко друг к другу, что указывает на отличную обобщающую способность модели.

![Пример ResNet-18](/NN_4_lab/image/result_ResNet-18.jpg)

### Заключение

Сравнение двух подходов однозначно показывает **подавляющее преимущество Transfer Learning** при работе с небольшими и средними наборами данных. Вместо того чтобы тратить ресурсы на обучение модели с нуля, гораздо эффективнее использовать знания, уже заложенные в предобученных моделях, и лишь адаптировать их под конкретную задачу. Этот метод позволяет достичь высокой точности значительно быстрее и с меньшими вычислительными затратами, избегая при этом проблемы переобучения.

![Пример ResNet-18](/NN_4_lab/image/results_plants.jpg)
