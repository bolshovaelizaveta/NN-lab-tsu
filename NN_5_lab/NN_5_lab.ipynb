{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEticeoLJcqn"
      },
      "outputs": [],
      "source": [
        "# Импорты\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmSgY2UFKirR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egVmHkHSKn1t"
      },
      "outputs": [],
      "source": [
        "# Путь к данным\n",
        "base_path = '/content/drive/MyDrive/my_colab_data/GTA5/Resized images'\n",
        "train_img_dir = os.path.join(base_path, 'Images/Train')\n",
        "train_mask_dir = os.path.join(base_path, 'Masks/Train')\n",
        "test_img_dir = os.path.join(base_path, 'Images/Test')\n",
        "test_mask_dir = os.path.join(base_path, 'Masks/Test')\n",
        "\n",
        "# Кастомный Dataset для сегментации\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, transform=None, mask_transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        self.image_files = sorted(os.listdir(img_dir))\n",
        "        self.valid_pairs = []\n",
        "\n",
        "        # Проверяем наличие пары для каждого изображения\n",
        "        for img_name in self.image_files:\n",
        "            mask_path = os.path.join(self.mask_dir, img_name) # Предполагаем, что имена совпадают\n",
        "            if os.path.exists(mask_path):\n",
        "                self.valid_pairs.append(img_name)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Используем имя файла из списка проверенных пар\n",
        "        img_name = self.valid_pairs[idx]\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.mask_transform:\n",
        "            mask = self.mask_transform(mask)\n",
        "            mask = torch.from_numpy(np.array(mask)).long()\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Трансформации\n",
        "transform_image = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_mask = transforms.Compose([\n",
        "    lambda img: img\n",
        "])\n",
        "\n",
        "# Создаем датасеты\n",
        "train_dataset = SegmentationDataset(train_img_dir, train_mask_dir, transform=transform_image, mask_transform=transform_mask)\n",
        "val_dataset = SegmentationDataset(test_img_dir, test_mask_dir, transform=transform_image, mask_transform=transform_mask)\n",
        "\n",
        "print(f\"Найдено {len(train_dataset)} валидных обучающих пар (изображение, маска)\")\n",
        "print(f\"Найдено {len(val_dataset)} валидных валидационных пар (изображение, маска)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfDkkVSKK441"
      },
      "outputs": [],
      "source": [
        "# Визуализация для проверки\n",
        "\n",
        "# Выведем несколько случайных пар\n",
        "num_images = 4\n",
        "random_indices = random.sample(range(len(train_dataset)), num_images)\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for i, idx in enumerate(random_indices):\n",
        "    image, mask = train_dataset[idx]\n",
        "\n",
        "    # Де-нормализация для отображения картинки\n",
        "    img_display = image.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img_display = std * img_display + mean\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "    # Отображаем картинку\n",
        "    plt.subplot(2, num_images, i + 1)\n",
        "    plt.imshow(img_display)\n",
        "    plt.title(f\"Изображение {idx}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Отображаем маску\n",
        "    plt.subplot(2, num_images, i + num_images + 1)\n",
        "    plt.imshow(mask)\n",
        "    plt.title(f\"Маска {idx}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMrQkFD2MEJk"
      },
      "outputs": [],
      "source": [
        "# Определение архитектуры U-Net\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    # Вспомогательная функция: (Conv2d -> ReLU) * 2\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Энкодер (сжимающий путь)\n",
        "        self.dconv_down1 = double_conv(3, 64)\n",
        "        self.dconv_down2 = double_conv(64, 128)\n",
        "        self.dconv_down3 = double_conv(128, 256)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "        # Боттлнек\n",
        "        self.bottleneck = double_conv(256, 512)\n",
        "\n",
        "        # Декодер (расширяющий путь)\n",
        "        self.upsample3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dconv_up3 = double_conv(256 + 256, 256)\n",
        "\n",
        "        self.upsample2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dconv_up2 = double_conv(128 + 128, 128)\n",
        "\n",
        "        # Финальный слой\n",
        "        self.final_conv = nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Энкодер\n",
        "        conv1 = self.dconv_down1(x)\n",
        "        x = self.maxpool(conv1)\n",
        "\n",
        "        conv2 = self.dconv_down2(x)\n",
        "        x = self.maxpool(conv2)\n",
        "\n",
        "        conv3 = self.dconv_down3(x)\n",
        "        x = self.maxpool(conv3)\n",
        "\n",
        "        # Боттлнек\n",
        "        bottleneck_out = self.bottleneck(x)\n",
        "\n",
        "        # Декодер + Skip Connections\n",
        "        x = self.upsample3(bottleneck_out)\n",
        "        x = torch.cat([x, conv3], dim=1)\n",
        "        x = self.dconv_up3(x)\n",
        "\n",
        "        x = self.upsample2(x)\n",
        "        x = torch.cat([x, conv2], dim=1)\n",
        "        x = self.dconv_up2(x)\n",
        "\n",
        "        # Финальный слой\n",
        "        out = self.final_conv(x)\n",
        "        out = F.interpolate(out, size=conv1.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"Архитектура SimpleUNet определена\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMq1Ax-MTgs"
      },
      "outputs": [],
      "source": [
        "# Обучение SimpleUNet с нуля\n",
        "import torch.nn.functional as F\n",
        "# Гиперпараметры\n",
        "batch_size = 8\n",
        "num_classes = 35\n",
        "num_epochs = 25\n",
        "lr = 0.001\n",
        "\n",
        "# Загрузчики данных\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_scratch = SimpleUNet(num_classes=num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_scratch.parameters(), lr=lr)\n",
        "\n",
        "history_scratch = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "print(\"Начинаем обучение SimpleUNet с нуля\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Фаза обучения\n",
        "    model_scratch.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Эпоха {epoch+1}/{num_epochs} [Обучение]\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_scratch(images)\n",
        "\n",
        "        # У масок может быть лишнее измерение [B, 1, H, W] - убираем\n",
        "        if masks.ndim == 4:\n",
        "            masks = masks.squeeze(1)\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_dataset)\n",
        "    history_scratch['train_loss'].append(epoch_train_loss)\n",
        "\n",
        "    # Фаза валидации\n",
        "    model_scratch.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            if masks.ndim == 4:\n",
        "                masks = masks.squeeze(1)\n",
        "            outputs = model_scratch(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_dataset)\n",
        "    history_scratch['val_loss'].append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Эпоха {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0e8UPhgMeui"
      },
      "outputs": [],
      "source": [
        "# Визуализация потерь\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(history_scratch['train_loss'], label='Потери на обучении')\n",
        "plt.plot(history_scratch['val_loss'], label='Потери на валидации')\n",
        "plt.title('Потери модели U-Net (с нуля)')\n",
        "plt.ylabel('Потери (CrossEntropy)')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65mm-7q1XDFU"
      },
      "outputs": [],
      "source": [
        "# Визуализация результатов SimpleUNet\n",
        "\n",
        "def visualize_predictions(model, dataset, num_images=5):\n",
        "    # Отображает исходное изображение, истинную маску и предсказание модели\n",
        "    model.eval()\n",
        "\n",
        "    # Берем случайные картинки из датасета\n",
        "    indices = random.sample(range(len(dataset)), num_images)\n",
        "\n",
        "    fig, axes = plt.subplots(3, num_images, figsize=(18, 9))\n",
        "    fig.suptitle(\"Результаты сегментации (U-Net с нуля)\", fontsize=20)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, true_mask = dataset[idx]\n",
        "\n",
        "            # Добавляем batch-измерение и отправляем на устройство\n",
        "            image_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "            # Получаем предсказание\n",
        "            output = model(image_tensor) # [1, num_classes, H, W]\n",
        "\n",
        "            # Преобразуем выход в маску\n",
        "            pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy() # [H, W]\n",
        "\n",
        "            # Исходное изображение\n",
        "            img_display = image.cpu().numpy().transpose((1, 2, 0))\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "            img_display = std * img_display + mean\n",
        "            img_display = np.clip(img_display, 0, 1)\n",
        "            axes[0, i].imshow(img_display)\n",
        "            axes[0, i].set_title(f\"Изображение {idx}\")\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "            # Истинная маска\n",
        "            axes[1, i].imshow(true_mask)\n",
        "            axes[1, i].set_title(f\"Истинная маска {idx}\")\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "            # Предсказанная маска\n",
        "            axes[2, i].imshow(pred_mask)\n",
        "            axes[2, i].set_title(f\"Предсказание {idx}\")\n",
        "            axes[2, i].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# Визуализация\n",
        "visualize_predictions(model_scratch, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90Lbi89YOh1"
      },
      "outputs": [],
      "source": [
        "# Определение архитектуры U-Net на базе VGG-16\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class VGGUNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGUNet, self).__init__()\n",
        "\n",
        "        # Загружаем предобученный VGG-16\n",
        "        vgg = vgg16(pretrained=True)\n",
        "\n",
        "        # Энкодер из VGG\n",
        "        self.encoder1 = vgg.features[:4]  # Conv1_1, ReLU, Conv1_2, ReLU\n",
        "        self.encoder2 = vgg.features[5:9]  # Conv2_1, ...\n",
        "        self.encoder3 = vgg.features[10:16] # Conv3_1, ...\n",
        "        self.encoder4 = vgg.features[17:23] # Conv4_1, ...\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Замораживаем веса энкодера, чтобы не переобучать\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Боттлнек\n",
        "        self.bottleneck = double_conv(512, 1024)\n",
        "\n",
        "        # Декодер\n",
        "        self.upsample4 = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
        "        self.dconv_up4 = double_conv(512 + 512, 512)\n",
        "\n",
        "        self.upsample3 = nn.ConvTranspose2d(512, 256, 2, 2)\n",
        "        self.dconv_up3 = double_conv(256 + 256, 256)\n",
        "\n",
        "        self.upsample2 = nn.ConvTranspose2d(256, 128, 2, 2)\n",
        "        self.dconv_up2 = double_conv(128 + 128, 128)\n",
        "\n",
        "        self.upsample1 = nn.ConvTranspose2d(128, 64, 2, 2)\n",
        "        self.dconv_up1 = double_conv(64 + 64, 64)\n",
        "\n",
        "        # Финальный слой\n",
        "        self.final_conv = nn.Conv2d(64, num_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Энкодер\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool(enc1))\n",
        "        enc3 = self.encoder3(self.pool(enc2))\n",
        "        enc4 = self.encoder4(self.pool(enc3))\n",
        "\n",
        "        # Боттлнек\n",
        "        bottleneck_out = self.bottleneck(self.pool(enc4))\n",
        "\n",
        "        # Декодер\n",
        "        dec4 = self.upsample4(bottleneck_out)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.dconv_up4(dec4)\n",
        "\n",
        "        dec3 = self.upsample3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.dconv_up3(dec3)\n",
        "\n",
        "        dec2 = self.upsample2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.dconv_up2(dec2)\n",
        "\n",
        "        dec1 = self.upsample1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.dconv_up1(dec1)\n",
        "\n",
        "        out = self.final_conv(dec1)\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"Архитектура VGGUNet определена\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mDnMK86IYPzt"
      },
      "outputs": [],
      "source": [
        "# Обучение VGGUNet\n",
        "\n",
        "# Гиперпараметры\n",
        "batch_size = 8\n",
        "num_classes = 35\n",
        "num_epochs = 25\n",
        "lr = 0.001\n",
        "\n",
        "# Загрузчики данных\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_vgg = VGGUNet(num_classes=num_classes).to(device)\n",
        "\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_vgg.parameters()), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "history_vgg = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "print(\"Начинаем обучение VGG-U-Net\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_vgg.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Эпоха {epoch+1}/{num_epochs} [Обучение]\"):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        if masks.ndim == 4: masks = masks.squeeze(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_vgg(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_dataset)\n",
        "    history_vgg['train_loss'].append(epoch_train_loss)\n",
        "\n",
        "    model_vgg.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            if masks.ndim == 4: masks = masks.squeeze(1)\n",
        "            outputs = model_vgg(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_dataset)\n",
        "    history_vgg['val_loss'].append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Эпоха {epoch+1}/{num_epochs} | \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JLyBa-WYXHw"
      },
      "outputs": [],
      "source": [
        "# Визуализация потерь VGG-U-Net\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(history_vgg['train_loss'], label='Потери на обучении')\n",
        "plt.plot(history_vgg['val_loss'], label='Потери на валидации')\n",
        "plt.title('Потери модели VGG-U-Net')\n",
        "plt.ylabel('Потери (CrossEntropy)')\n",
        "plt.xlabel('Эпоха')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bevw7rXXYZwH"
      },
      "outputs": [],
      "source": [
        "# Визуализация результатов VGG-U-Net\n",
        "visualize_predictions(model_vgg, val_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}